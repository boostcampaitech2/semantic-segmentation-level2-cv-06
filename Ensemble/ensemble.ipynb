{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from importlib import import_module\n",
    "\n",
    "import torch\n",
    "from dataset import CustomDataLoader, collate_fn, test_transform\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './output'\n",
    "\n",
    "# load sample_submission.csv\n",
    "submission = pd.read_csv('../baseline_code/submission/sample_submission.csv', index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start prediction..\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "print(\"Start prediction..\")\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "test_dataset = CustomDataLoader(data_dir='/opt/ml/segmentation/input/data/test.json', mode='test', transform=test_transform)\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=8,\n",
    "    num_workers=4,\n",
    "    shuffle=False,\n",
    "    pin_memory=use_cuda,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ttach as tta\n",
    "transforms = tta.Compose(\n",
    "[\n",
    "    tta.HorizontalFlip(),\n",
    "    tta.Rotate90(angles=[0, 180]),\n",
    "    # tta.Resize(sizes=(512,512)),\n",
    "    # tta.Scale(scales=[1, 2, 4]),\n",
    "    tta.Multiply(factors=[0.9, 1, 1.1]),      \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_pretrained: grid-size from 24 to 32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SegmentationTTAWrapper(\n",
       "  (model): TransUnet(\n",
       "    (backbone): VisionTransformer(\n",
       "      (transformer): Transformer(\n",
       "        (embeddings): Embeddings(\n",
       "          (hybrid_model): ResNetV2(\n",
       "            (root): Sequential(\n",
       "              (conv): StdConv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "              (gn): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "              (relu): ReLU(inplace=True)\n",
       "            )\n",
       "            (body): Sequential(\n",
       "              (block1): Sequential(\n",
       "                (unit1): PreActBottleneck(\n",
       "                  (gn1): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "                  (conv1): StdConv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (gn2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "                  (conv2): StdConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (gn3): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                  (conv3): StdConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (relu): ReLU(inplace=True)\n",
       "                  (downsample): StdConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (gn_proj): GroupNorm(256, 256, eps=1e-05, affine=True)\n",
       "                )\n",
       "                (unit2): PreActBottleneck(\n",
       "                  (gn1): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "                  (conv1): StdConv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (gn2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "                  (conv2): StdConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (gn3): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                  (conv3): StdConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (relu): ReLU(inplace=True)\n",
       "                )\n",
       "                (unit3): PreActBottleneck(\n",
       "                  (gn1): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "                  (conv1): StdConv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (gn2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
       "                  (conv2): StdConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (gn3): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                  (conv3): StdConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (relu): ReLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (block2): Sequential(\n",
       "                (unit1): PreActBottleneck(\n",
       "                  (gn1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "                  (conv1): StdConv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (gn2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "                  (conv2): StdConv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                  (gn3): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "                  (conv3): StdConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (relu): ReLU(inplace=True)\n",
       "                  (downsample): StdConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                  (gn_proj): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "                )\n",
       "                (unit2): PreActBottleneck(\n",
       "                  (gn1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "                  (conv1): StdConv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (gn2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "                  (conv2): StdConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (gn3): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "                  (conv3): StdConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (relu): ReLU(inplace=True)\n",
       "                )\n",
       "                (unit3): PreActBottleneck(\n",
       "                  (gn1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "                  (conv1): StdConv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (gn2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "                  (conv2): StdConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (gn3): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "                  (conv3): StdConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (relu): ReLU(inplace=True)\n",
       "                )\n",
       "                (unit4): PreActBottleneck(\n",
       "                  (gn1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "                  (conv1): StdConv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (gn2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "                  (conv2): StdConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (gn3): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "                  (conv3): StdConv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (relu): ReLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "              (block3): Sequential(\n",
       "                (unit1): PreActBottleneck(\n",
       "                  (gn1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                  (conv1): StdConv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (gn2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                  (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                  (gn3): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
       "                  (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (relu): ReLU(inplace=True)\n",
       "                  (downsample): StdConv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                  (gn_proj): GroupNorm(1024, 1024, eps=1e-05, affine=True)\n",
       "                )\n",
       "                (unit2): PreActBottleneck(\n",
       "                  (gn1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                  (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (gn2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                  (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (gn3): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
       "                  (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (relu): ReLU(inplace=True)\n",
       "                )\n",
       "                (unit3): PreActBottleneck(\n",
       "                  (gn1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                  (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (gn2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                  (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (gn3): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
       "                  (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (relu): ReLU(inplace=True)\n",
       "                )\n",
       "                (unit4): PreActBottleneck(\n",
       "                  (gn1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                  (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (gn2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                  (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (gn3): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
       "                  (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (relu): ReLU(inplace=True)\n",
       "                )\n",
       "                (unit5): PreActBottleneck(\n",
       "                  (gn1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                  (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (gn2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                  (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (gn3): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
       "                  (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (relu): ReLU(inplace=True)\n",
       "                )\n",
       "                (unit6): PreActBottleneck(\n",
       "                  (gn1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                  (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (gn2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                  (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (gn3): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
       "                  (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (relu): ReLU(inplace=True)\n",
       "                )\n",
       "                (unit7): PreActBottleneck(\n",
       "                  (gn1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                  (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (gn2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                  (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (gn3): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
       "                  (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (relu): ReLU(inplace=True)\n",
       "                )\n",
       "                (unit8): PreActBottleneck(\n",
       "                  (gn1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                  (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (gn2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                  (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (gn3): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
       "                  (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (relu): ReLU(inplace=True)\n",
       "                )\n",
       "                (unit9): PreActBottleneck(\n",
       "                  (gn1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                  (conv1): StdConv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (gn2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "                  (conv2): StdConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (gn3): GroupNorm(32, 1024, eps=1e-06, affine=True)\n",
       "                  (conv3): StdConv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (relu): ReLU(inplace=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (patch_embeddings): Conv2d(1024, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): Encoder(\n",
       "          (layer): ModuleList(\n",
       "            (0): Block(\n",
       "              (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (ffn): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "            )\n",
       "            (1): Block(\n",
       "              (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (ffn): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "            )\n",
       "            (2): Block(\n",
       "              (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (ffn): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "            )\n",
       "            (3): Block(\n",
       "              (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (ffn): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "            )\n",
       "            (4): Block(\n",
       "              (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (ffn): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "            )\n",
       "            (5): Block(\n",
       "              (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (ffn): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "            )\n",
       "            (6): Block(\n",
       "              (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (ffn): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "            )\n",
       "            (7): Block(\n",
       "              (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (ffn): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "            )\n",
       "            (8): Block(\n",
       "              (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (ffn): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "            )\n",
       "            (9): Block(\n",
       "              (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (ffn): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "            )\n",
       "            (10): Block(\n",
       "              (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (ffn): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "            )\n",
       "            (11): Block(\n",
       "              (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "              (ffn): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (attn): Attention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (decoder): DecoderCup(\n",
       "        (conv_more): Conv2dReLU(\n",
       "          (0): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (blocks): ModuleList(\n",
       "          (0): DecoderBlock(\n",
       "            (conv1): Conv2dReLU(\n",
       "              (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (conv2): Conv2dReLU(\n",
       "              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (up): UpsamplingBilinear2d(scale_factor=2.0, mode=bilinear)\n",
       "          )\n",
       "          (1): DecoderBlock(\n",
       "            (conv1): Conv2dReLU(\n",
       "              (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (conv2): Conv2dReLU(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (up): UpsamplingBilinear2d(scale_factor=2.0, mode=bilinear)\n",
       "          )\n",
       "          (2): DecoderBlock(\n",
       "            (conv1): Conv2dReLU(\n",
       "              (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (conv2): Conv2dReLU(\n",
       "              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (up): UpsamplingBilinear2d(scale_factor=2.0, mode=bilinear)\n",
       "          )\n",
       "          (3): DecoderBlock(\n",
       "            (conv1): Conv2dReLU(\n",
       "              (0): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (conv2): Conv2dReLU(\n",
       "              (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "            (up): UpsamplingBilinear2d(scale_factor=2.0, mode=bilinear)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (segmentation_head): SegmentationHead(\n",
       "        (0): Conv2d(16, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Identity()\n",
       "      )\n",
       "    )\n",
       "    (ce_loss): CrossEntropyLoss()\n",
       "    (dice_loss): DiceLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    models_name = ['DeepLabV3_Plus', 'TransUnet']\n",
    "    ckpt_list = ['/opt/ml/segmentation/semantic-segmentation-level2-cv-06/runs/deeplab_v3_b7_best.pt',\n",
    "                '/opt/ml/segmentation/semantic-segmentation-level2-cv-06/runs/transunet_SGD_0.61_0.63.pt']\n",
    "    \n",
    "    # model 1 가중치 불러오기\n",
    "    model_module = getattr(import_module(\"models.model\"), models_name[0])\n",
    "    model1 = model_module(num_classes=11, pretrained=True)\n",
    "    checkpoint = torch.load(ckpt_list[0], map_location=device)\n",
    "    state_dict = checkpoint.state_dict()\n",
    "    model1.load_state_dict(state_dict)\n",
    "    model1.to(device)\n",
    "\n",
    "    # model 2 가중치 불러오기\n",
    "    model_module = getattr(import_module(\"models.model\"), models_name[1])\n",
    "    model2 = model_module(num_classes=11, pretrained=True)\n",
    "    checkpoint = torch.load(ckpt_list[1], map_location=device)\n",
    "    state_dict = checkpoint.state_dict()\n",
    "    model2.load_state_dict(state_dict)\n",
    "    model2.to(device)\n",
    "\n",
    "    # wrapper로 TTA augmentation 적용\n",
    "    tta_model1 = tta.SegmentationTTAWrapper(model1, transforms)\n",
    "    tta_model2 = tta.SegmentationTTAWrapper(model2, transforms)\n",
    "    \n",
    "    # 각 모델 eval로 type 변경\n",
    "    model1.eval()\n",
    "    tta_model1.eval()\n",
    "    model2.eval()\n",
    "    tta_model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 256\n",
    "transform = A.Compose([A.Resize(size, size)])\n",
    "\n",
    "file_name_list = []\n",
    "preds_array = np.empty((0, size*size), dtype=np.compat.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/103 [00:00<?, ?it/s]/opt/conda/envs/segmentation/lib/python3.7/site-packages/torch/nn/functional.py:3063: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
      "/opt/conda/envs/segmentation/lib/python3.7/site-packages/torch/nn/functional.py:3103: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor changed \"\n",
      "100%|██████████| 103/103 [42:48<00:00, 20.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End prediction!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    " with torch.no_grad():\n",
    "    for step, (imgs, image_infos) in enumerate(tqdm(test_loader)):\n",
    "        # inference (512 x 512)\n",
    "        outs = 0\n",
    "        for model_name in models_name:\n",
    "            if model_name in ('FCNRes50', 'FCNRes101', 'DeepLabV3_Res50', 'DeepLabV3_Res101'):\n",
    "                # output size : ([8, 11, 512, 512])\n",
    "                outs += model1(torch.stack(imgs).to(device))['out']\n",
    "                outs += tta_model1(torch.stack(imgs).to(device))['out']\n",
    "                # print('if : ', model_name)\n",
    "            else:\n",
    "                # 원하는 모델을 더해주고 더해준 모델의 개수만큼 나눠준다.\n",
    "                images = torch.stack(imgs).to(device)\n",
    "                \n",
    "                outs += model1(images)\n",
    "                outs += tta_model1(images)\n",
    "\n",
    "                # transunet input size (1024,1024)로 변경하여 넣어주기\n",
    "                image_ = []\n",
    "                for i in imgs:\n",
    "                    i = i.squeeze()\n",
    "                    i = torch.transpose(i, 0,2)\n",
    "                    i_ = cv2.resize(np.float32(i), (1024,1024), interpolation = cv2.INTER_CUBIC)\n",
    "                    i_ = torch.transpose(torch.tensor(i_), 0,2)\n",
    "                    image_.append(i_)\n",
    "                transunet_image = torch.stack(image_).to(device)\n",
    "\n",
    "                outs += model2(transunet_image)\n",
    "                outs += tta_model2(transunet_image)\n",
    "\n",
    "                # print('else : ', model_name)\n",
    "                # outs = (1/4)*(model1(images)+tta_model1(images)+model2(images)+tta_model2(images))   \n",
    "            outs = (1/4)*outs\n",
    " \n",
    "        oms = torch.argmax(outs.squeeze(), dim=1).detach().cpu().numpy() # (8, 512, 512)\n",
    "\n",
    "        # resize (256 x 256)\n",
    "        temp_mask = []\n",
    "        for img, mask in zip(np.stack(imgs), oms):\n",
    "            transformed = transform(image=img, mask=mask)\n",
    "            mask = transformed['mask']\n",
    "            temp_mask.append(mask)\n",
    "\n",
    "\n",
    "        oms = np.array(temp_mask)\n",
    "        oms = oms.reshape([oms.shape[0], size*size]).astype(int)\n",
    "        preds_array = np.vstack((preds_array, oms))\n",
    "\n",
    "        file_name_list.append([i['file_name'] for i in image_infos])\n",
    "\n",
    "print(\"End prediction!\")\n",
    "file_names = [y for x in file_name_list for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save\n",
    "with open('data.pickle', 'wb') as f:\n",
    "    pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# load\n",
    "with open('data.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write PredictionString\n",
    "for file_name, string in zip(file_names, preds_array):\n",
    "    submission = submission.append({\"image_id\": file_name, \"PredictionString\": ' '.join(str(e) for e in string.tolist())},\n",
    "                                    ignore_index=True)\n",
    "\n",
    "# save submission.csv\n",
    "submission.to_csv(output_dir+'/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segmentation",
   "language": "python",
   "name": "segmentation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
